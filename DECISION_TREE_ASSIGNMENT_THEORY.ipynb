{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM5dnsiqwsprFcqZ6zQBB+c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/devpandey2010/DECISION-TREE/blob/main/DECISION_TREE_ASSIGNMENT_THEORY.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4PPsJFauMmwO"
      },
      "outputs": [],
      "source": [
        "#Q1.What is a Decision Tree, and how does it work?\n",
        "'''>>decision tree is algorithm in machine learning to solve non linear classification and regression problem.It works on nested if -else condition.\n",
        "     Decision tree divides the impure data into pockets wich is also called nodes in a recursive way until it gets a pure node.this gives it a structure of tree thats why it calles as tree.'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q2. What are impurity measures in Decision Trees\n",
        "'''>>In decision tree each node is spliltted until it gets a pure node this split depends on the impurity measures.There are three impurity measures\n",
        "     i.classification error >>it means if i assign every element to the majority class then what is the error rate.\n",
        "     ii.Gini >>Measurs how often a randomly choosen sample is incorrectly classified.It can be interpreted as the expected error rate in a classifier.\n",
        "     iii.Entropy >> it measurs a randomness in the data,lower entropy means better homogeneity in data.It can be interpreted as the average amount of information needed to specify the class of an instance.\n",
        "     iV.variance reduction\n",
        "     V.variance"
      ],
      "metadata": {
        "id": "CkBkGr8KMuuG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q3.What is the mathematical formula for Gini Impurity\n",
        "'''>>the mathematical formula >> 1-(summation from i=0 to i=n-1 (pi)^2)\n",
        "     where pi>>probability of ith element.\n",
        "     its range from 0 to 0.5.'''\n"
      ],
      "metadata": {
        "id": "nk46bbbQMuxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q4.What is the mathematical formula for Entropy\n",
        "'''>> the mathematical formula >>-(summation from i=0 to i=n-1 pi*logpi with base 2)\n",
        "       its range from 0 to 1'''"
      ],
      "metadata": {
        "id": "l6Eq5-b7Mu02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q5.What is Information Gain, and how is it used in Decision Trees\n",
        "'''>> Information Gain helps in determining the best feature for splitting.\n",
        "      In simpler terms, Information Gain helps us understand how much a particular feature contributes to making accurate predictions in a decision tree.\n",
        "      Features with higher Information Gain are considered more informative and are preferred for splitting the dataset, as they lead to nodes with more homogenous classes.\n",
        "      Information gain=pre(impurity)-post(impurity).'''\n"
      ],
      "metadata": {
        "id": "oiLdRF4jMu4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q6. What is the difference between Gini Impurity and Entropy\n",
        "''' >>Gini can be interpreted as the expected error rate in classification.\n",
        "    >>entropy can be interpreted as the average amount of information needed to specify the class of the instance\n",
        "\n",
        "    >>Gini is comparartively faster than entopy as entropy has lograthmic which is more computationaly expensive.\n",
        "\n",
        "    >>Gini has bias towards selecting splits that results in a more balanced distribution of classes.\n",
        "    >>entropy has bias towards selecting splits that result in higher reduction of uncertainity.\n",
        "\n",
        "     >>Gini is used in algo like CART algo\n",
        "     >>entropy is used in algo like ID3,C4.5 algo '''\n",
        "\n",
        "\n",
        "#Q7.What is the mathematical explanation behind Decision Trees\n",
        "'''>>while doing Decision tree we are facing two major Questions\n",
        "    i.how split will happen\n",
        "    ii.which feature is best for splitting\n",
        "    the answers are given by impurity measure and information gain\n",
        "    at each node we will calculate this impurity measure and information gain\n",
        "    and then split will happen as stated above all the formulas of impurity measure and information gain,\n",
        "    At each split it has a aim of attaing pure node that is increasibg purity and this split happen in recursive manner\n",
        "    until stopping criteria is reached.\n",
        "    we also do to prepunning and postpunning to avoid overfitting.'''\n",
        "\n",
        "\n",
        "\n",
        "#Q8. What is Pre-Pruning in Decision Trees\n",
        "'''>>Its is a method in Decision tree to reduce overfitting.It limits the growth of decision Tree reaches its full potential\n",
        "     Maximum Depth: It limits the maximum level of depth in a decision tree.\n",
        "     Minimum Samples per Leaf: Set a minimum threshold for the number of samples in each leaf node.\n",
        "     Minimum Samples per Split: Specify the minimal number of samples needed to break up a node.\n",
        "    Maximum Features: Restrict the quantity of features considered for splitting.'''\n",
        "\n",
        "\n",
        "\n",
        " #Q9.What is Post-Pruning in Decision Trees\n",
        " '''>> it  is a method in Decision Tree to  reduce overfitting. It works on cutting or prunning of tree after it is created fully\n",
        "       it is used only for smaller dataset.Because it is computationally expensive.'''\n",
        "\n",
        "\n",
        "  #Q10.What is the difference between Pre-Pruning and Post-Pruning?\n",
        "  '''>>i.pre punning limits the growth of the tree before it is completely grown.\n",
        "         post prunning cutoff the nodes of the tree after its is completely grown.\n",
        "\n",
        "      ii.prepunning is faster and less computationaly expensive than post prunning and used in larger as well as smaller dataset.\n",
        "         postprunning is computationaly expensive and used only for smaller dataset.'''\n",
        "\n",
        "\n",
        "#Q11.What is a Decision Tree Regressor?\n",
        "'''>>Decision Tree Regression is a non-linear regression method that can handle complex datasets with intricate patterns.\n",
        "     It uses a tree-like model to make predictions, making it both flexible and easy to interpret.\n",
        "     Decision Tree Regression predicts continuous values. It does this by splitting the data into smaller subsets based on decision rules derived from the input features.\n",
        "     Each split is made to minimize the error in predicting the target variable.\n",
        "      At leaf node of the tree the model predicts a continuous value which is typically the average of the target values in that node.'''\n",
        "\n",
        " #Q12.What are the advantages and disadvantages of Decision Trees\n",
        "  '''>>ADVANTAGES:\n",
        "     *i.simple to undersatnd.\n",
        "      ii.captures non linear data.\n",
        "      iii.it divides the data into smaller subset.\n",
        "\n",
        "    >>Disadvantage\n",
        "     i.Training time si more.\n",
        "     ii.Decision tree is a greedy algorithm .it keeps spliiting until every leaf node is pure which leads to overfiiting.'''\n",
        "\n",
        "#Q13.How does a Decision Tree handle missing values?\n",
        "'''>>Decision trees handle missing data by either ignoring instances with missing values, imputing them using statistical measures,\n",
        "     or creating separate branches. During prediction, the tree follows the training strategy, applying imputation or navigating a dedicated branch for instances with missing data.'''\n",
        "\n",
        "\n",
        "#Q14.How does a Decision Tree handle categorical features?\n",
        "'''>>Decision trees handle categorical features by converting them to numbers, or by using methods that handle categorical data natively.\n",
        "Convert categorical features to numbers\n",
        "One-hot encoding: Transforms each category into a binary vector. This is useful when there is no inherent order among the categories.\n",
        "Label encoding: Maps each unique category to an integer.\n",
        "Target encoding: Replaces categorical values with the mean of the target variable for each category.'''\n",
        "\n",
        "\n",
        "#Q15.What are some real-world applications of Decision Trees?\n",
        "'''>>Loan Approval in Banking: A bank needs to decide whether to approve a loan application based on customer profiles.\n",
        "Input features include income, credit score, employment status, and loan history.\n",
        "The decision tree predicts loan approval or rejection, helping the bank make quick and reliable decisions.\n",
        "\n",
        "Medical Diagnosis: A healthcare provider wants to predict whether a patient has diabetes based on clinical test results.\n",
        "Features like glucose levels, BMI, and blood pressure are used to make a decision tree.\n",
        "Tree classifies patients into diabetic or non-diabetic, assisting doctors in diagnosis.\n",
        "\n",
        "Predicting Exam Results in Education : School wants to predict whether a student will pass or fail based on study habits.\n",
        "Data includes attendance, time spent studying, and previous grades.\n",
        "The decision tree identifies at-risk students, allowing teachers to provide additional support.'''"
      ],
      "metadata": {
        "id": "XpPKaQc7Mu7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4jZF0BkcMvCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ge8ZtQXrMvFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V5JVpfLBMvJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5LftjF2pMvMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SCbj_HVTMv2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1vB1nkHdMv6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "stQfmRI8Mv9u"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}